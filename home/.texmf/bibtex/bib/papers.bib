@inproceedings{Chen1996,
address = {Morristown, NJ, USA},
author = {Chen, Stanley F. and Goodman, Joshua},
booktitle = {Proceedings of the 34th annual meeting on Association for Computational Linguistics -},
doi = {10.3115/981863.981904},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1996 - An empirical study of smoothing techniques for language modeling.pdf:pdf},
pages = {310--318},
publisher = {Association for Computational Linguistics},
title = {{An empirical study of smoothing techniques for language modeling}},
year = {1996}
}
@article{Uematsu2018,
author = {大輝, 上松 and 麗花, 趙 and Kertkeidkachorn, Natthawut and 龍太郎, 市瀬},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/大輝 et al. - 2018 - オントロジーマッチングを用いた知識グラフの構築.pdf:pdf},
journal = {人工知能学会研究会資料},
month = {mar},
number = {4},
pages = {1--5},
title = {オントロジーマッチングを用いた知識グラフの構築},
volume = {44},
year = {2018}
}
@inproceedings{Nakano2016,
abstract = {Good-Turing のスムージングとナイーブベイズを用いた先行研究において,名前の周辺と名前を 構成する文字列を特徴量としている抽出方法がある.本研究では Good-Turing の代わりに信頼区間の下限値に よる確率推定を用いた企業名抽出を提案する.先行研究と同様の条件で新聞記事から企業名の抽出を行う比較 実験を行なった結果,近似された適合率及び近似された再現率のそれぞれにおいて提案手法が Good-Turing を 用いた方法を上回り,有意水準 1{\%}で提案手法と先行研究の有意差が認められた.},
author = {Nakano, Shohei and Kikuchi, Masato and Yoshida, Mitsuo and Okabe, Masayuki and Umemura, Kyoji},
booktitle = {DEIM Forum 2016},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nakano et al. - 2016 - 信頼区間の下限値による確率推定を用いた企業名抽出.pdf:pdf},
pages = {435--436},
title = {信頼区間の下限値による確率推定を用いた企業名抽出},
year = {2016}
}
@inproceedings{Kikuchi2018,
abstract = {自然言語処理におけるブートストラップ法は,それぞれの繰り返しにおいて確実な分類結果を次の学習に 使用することが求められる.ここで,尤度比に基づいて分類結果を得るとすると,尤度比の保守的な推定量を使用す る方法が考えられる.本稿では,尤度比の直接推定法で導入される正則化項の扱いで保守的な推定量が得られること を指摘し,保守的な推定量とブートストラップ法を組み合わせることで分類性能が向上したことを報告する.},
author = {真人,菊地 and 光男,吉田 and 恭司,梅村},
booktitle = {DEIM Forum 2018},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kikuchi, Yoshida, Umemura - 2018 - ブートストラップに適した尤度比の保守的な直接推定法.pdf:pdf},
title = {ブートストラップに適した尤度比の保守的な直接推定法},
year = {2018}
}
@inproceedings{Kawakami2018,
author = {Kawakami, Kento and Kikuchi, Masato and Yoshida, Mitsuo and Umemura, Kyoji},
booktitle = {ICAICTA2018},
file = {:home/s173342/angel/ulab{\_}share/Publications/2018/201808 - ICAICTA2018/川上/PID5486185.pdf:pdf},
title = {{Direct Estimation of Likelihood Ratio for the Analysis of Context}},
year = {2018}
}
@inproceedings{Kawakami2017,
author = {Kawakami, Kento and Kikuchi, Masato and Yoshida, Mitsuo and Yamamoto, Eiko and Umemura, Kyoji},
booktitle = {2017 International Conference on Advanced Informatics, Concepts, Theory, and Applications (ICAICTA)},
doi = {10.1109/ICAICTA.2017.8090969},
file = {:home/s173342/angel/ulab{\_}share/Publications/2017/201708 - ICAICTA2017/川上/PID4931365.pdf:pdf},
isbn = {978-1-5386-3001-3},
month = {aug},
pages = {1--5},
publisher = {IEEE},
title = {{Finding association rules by direct estimation of likelihood ratios}},
year = {2017}
}
@article{Gale1995,
abstract = {The performance of statistically based techniques for many tasks such as spelling correction, sense disambiguation, and translation is improved if one can estimate a probability for an object of interest which has not been seen before. Good-Turing methods are one means of estimating these probabilities for previously unseen objects. However, the use of Good-Turing methods requires a smoothing step which must smooth in regions of vastly different accuracy. Such smoothers are difficult to use, and may have hindered the use of Good-Turing methods in computational linguistics. This paper presents a method which uses the simplest possible smooth, a straight line, together with a rule for switching from Turing estimates which are more accurate at low frequencies. We call this method the Simple Good-Turing (SGT) method. Two examples, one from prosody, the other from morphology, are used to illustrate the SGT. While the goal of this research was to provide a simple estimator, the SGT turns out to be the most accurate of several methods applied in a set of Monte Carlo examples which satisfy the assumptions of the Good- Turing methods. The accuracy of the SGT is compared to two other methods for estimating the same probabilities, the Expected Likelihood Estimate (ELE) and two way cross validation. The SGT method is more complex than ELE but simpler than two way cross validation. On a set of twenty Monte Carlo examples spanning vocabulary sizes from 5,000 to 100,000 and Zipfian coefficients from -1.4 to -1.1, the SGT was far more accurate than ELE. It was more accurate than two way cross validation for frequencies of two or less, and of approximately the same accuracy for higher frequencies. The importance of using the internal structure of objects to get differing estimates for the probabilities of unseen objects is discussed, and two examples show possible approaches. Appendices present the complete data needed to apply Good-Turing methods to the prosody and Chinese plurals examples, and code that implements the SGT.},
author = {Gale, William A. and Sampson, Geoffrey},
doi = {10.1080/09296179508590051},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gale, Sampson - 1995 - Good‐turing frequency estimation without tears.pdf:pdf},
issn = {0929-6174},
journal = {Journal of Quantitative Linguistics},
month = {jan},
number = {3},
pages = {217--237},
title = {{Good‐turing frequency estimation without tears}},
volume = {2},
year = {1995}
}
@inproceedings{Kikuchi2015,
author = {Kikuchi, Masato and Yoshida, Mitsuo and Okabe, Masayuki and Umemura, Kyoji},
file = {:home/s173342/angel/ulab{\_}share/Data/kikuchi14/B4/201508 - ICAICTA2015 - 菊地真人/PID3841023.pdf:pdf},
isbn = {9781467381437},
keywords = {class j,confidence interval,doc,is also,numerical integral,of p,smoothing,that is the product,word i,ˆ},
title = {{Confidence Interval of Probability Estimator of Laplace Smoothing}},
year = {2015}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
year = {2014}
}
@article{Hashimi2015,
abstract = {Text mining techniques include categorization of text, summarization, topic detection, concept extraction, search and retrieval, document clustering, etc. Each of these techniques can be used in finding some non-trivial information from a collection of documents. Text mining can also be employed to detect a document's main topic/theme which is useful in creating taxonomy from the document collection. Areas of applications for text mining include publishing, media, telecommunications, marketing, research, healthcare, medicine, etc. Text mining has also been applied on many applications on the World Wide Web for developing recommendation systems. We propose here a set of criteria to evaluate the effectiveness of text mining techniques in an attempt to facilitate the selection of appropriate technique.},
author = {Hashimi, Hussein and Hafez, Alaaeldin and Mathkour, Hassan},
doi = {10.1016/j.chb.2014.10.062},
isbn = {07475632},
issn = {07475632},
journal = {Computers in Human Behavior},
title = {{Selection criteria for text mining approaches}},
year = {2015}
}
@article{Saito2017,
abstract = {ソーシャルメディア等の崩れた日本語の解析においては，形態素解析辞書に存在しない語が多く出現するため解析誤りが新聞等のテキストに比べ増加する．辞書に存在しない未知語の中でも，既知の辞書語からの派生に関しては，正規形を考慮しながら解析するという表記正規化との同時解析の有効性が確認されている．本研究では，これまで焦点があてられていなかった，文字列の正規化パタン獲得に着目し，アノテーションデータから文字列の正規化パタンを統計的に抽出する．統計的に抽出した文字列正規化パタンと文字種正規化を用いて辞書語の候補を拡張し形態素解析を行った結果，従来法よりも再現率，精度ともに高い解析結果を得ることができた． Social media texts are often written in a non-standard style and include many lexi-cal variants such as insertions, phonetic substitutions, and abbreviations that mimic spoken language. The normalization of such a variety of non-standard tokens is one promising solution for handling noisy text. A normalization task is very difficult for the morphological analysis of Japanese text because there are no explicit bound-aries between words. To address this issue, we propose a novel method herein for normalizing and morphologically analyzing Japanese noisy text. First, we extract character-level transformation patterns based on a character alignment model using annotated data. Next, we generate both character-level and word-level normaliza-tion candidates using character transformation patterns and search for the optimal path based on a discriminative model. Experimental results show that the proposed method exceeds conventional rule-based system in both accuracy and recall for word segmentation and POS (Part of Speech) tagging.},
author = {Saito, Itsumi and Sadamitsu, Kugatsu and Asano, Hisako and Matsuo, Yoshihiro},
doi = {10.5715/jnlp.24.297},
file = {:home/s173342/Downloads/24{\_}297.pdf:pdf},
isbn = {9781941643266},
issn = {1340-7619},
journal = {Journal of Natural Language Processing},
keywords = {形態素解析,文字列アライメント,表記正規化},
number = {2},
pages = {297--314},
title = {{Morphological Analysis for Japanese Noisy Text based on Extraction of Character Transformation Patterns and Lexical Normalization}},
volume = {24},
year = {2017}
}
@article{Nagao1993,
author = {眞, 長尾 and 信介, 森},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/眞, 信介 - 1993 - 大規模日本語テキストのnグラム統計の作り方と語句の自動抽出.pdf:pdf},
journal = {情報処理学会研究報告自然言語処理（NL）},
month = {jul},
number = {61(1993-NL-096)},
pages = {1--8},
title = {大規模日本語テキストのnグラム統計の作り方と語句の自動抽出},
volume = {1993},
year = {1993}
}
@incollection{Bartoli2015,
author = {Bartoli, Alberto and {De Lorenzo}, Andrea and Medvet, Eric and Tarlao, Fabiano},
doi = {10.1007/978-3-319-16501-1_2},
file = {:home/s173342/Downloads/Bartoli2015{\_}Chapter{\_}LearningTextPatternsUsingSepar.pdf:pdf},
pages = {16--27},
publisher = {Springer, Cham},
title = {{Learning Text Patterns Using Separate-and-Conquer Genetic Programming}},
year = {2015}
}
@article{Bartoli2016,
author = {Bartoli, Alberto and {De Lorenzo}, Andrea and Medvet, Eric and Tarlao, Fabiano},
doi = {10.1109/TKDE.2016.2515587},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bartoli et al. - 2016 - Inference of Regular Expressions for Text Extraction from Examples.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = {may},
number = {5},
pages = {1217--1230},
title = {{Inference of Regular Expressions for Text Extraction from Examples}},
volume = {28},
year = {2016}
}
@techreport{Sekine2004,
abstract = {The tagging of Named Entities, the names of particular things or classes, is regarded as an important component technology for many NLP applications. The first Named Entity set had 7 types, organization, location, person, date, time, money and percent expressions. Later, in the IREX project artifact was added and ACE added two, GPE and facility, to pursue the generalization of the technology. However, 7 or 8 kinds of NE are not broad enough to cover general applications. We proposed about 150 categories of NE (Sekine et al. 2002) and now we have extended it again to 200 categories. Also we have developed dictionaries and an automatic tagger for NEs in Japanese.},
author = {Sekine, Satoshi and Nobata, Chikashi},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekine, Nobata - 2004 - Definition, dictionaries and tagger for Extended Named Entity Hierarchy.pdf:pdf},
pages = {1977--1980},
title = {{Definition, dictionaries and tagger for Extended Named Entity Hierarchy}},
year = {2004}
}
@article{Mori1995,
author = {信介, 森 and 眞, 長尾},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/信介, 眞 - 1995 - nグラム統計によるコーパスからの未知語抽出.pdf:pdf},
journal = {情報処理学会研究報告自然言語処理（NL）},
month = {jul},
number = {69(1995-NL-108)},
pages = {7--12},
title = {nグラム統計によるコーパスからの未知語抽出},
volume = {1995},
year = {1995}
}
@article{Nagata1999,
abstract = {本論文では, 統計的言語モデルとN-best探索アルゴリズムを用いた新しい日本語形態素解析法を提案する. 本方法は, 未知語の確率モデルを持つことにより任意の日本語文を高精度に解析し, 確率が大きい順に任意個の形態素解析候補を求められる. EDRコーパスの部分集合(約19万文, 約470万語)を用いて言語モデルの学習を行い, オープンテキスト100文に対してテストを行ったところ, 単語分割の精度は第1候補で再現率94.6{\%}適合率93.5{\%}, 上位五候補で再現率97.8{\%}適合率88.3{\%}であった.},
author = {永田, 昌明},
issn = {03875806},
journal = {情報処理学会論文誌},
title = {{統計的言語モデルとN-best探索を用いた日本語形態素解析法}},
year = {1999}
}
@inproceedings{Mochihashi2016,
abstract = {Hidden Markov models (HMM) is widely used in statistics and machine learning. However, it cannot learn latent states where these states are actually structured. Extending the tree-structured stick breaking processes (Adams+ 2010) hierarchically as from DP to HDP, this paper proposes an Infinite Tree Hidden Markov models (iTHMM) whose states constitute a latent hierarchy. Experimental results on natural language texts show the validity of the proposed algorithm.},
author = {持橋大地 and 能地宏},
booktitle = {情報処理学会研究報告},
title = {{無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習}},
year = {2016}
}
@article{Mochihashi2007,
author = {大地, 持橋 and 英一郎, 隅田},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/大地, 英一郎 - 2007 - Pitman-Yor 過程に基づく可変長 n-gram 言語モデル.pdf:pdf},
journal = {情報処理学会研究報告自然言語処理（NL）},
month = {mar},
number = {35(2007-NL-178)},
pages = {63--70},
title = {{Pitman-Yor 過程に基づく可変長 n-gram 言語モデル}},
volume = {2007},
year = {2007}
}
@article{TEH2006,
author = {TEH, Yee Whye and Whye, Yee},
file = {:home/s173342/Downloads/TRA2-06.pdf:pdf},
keywords = {Technical Report},
title = {{A Bayesian Interpretation of Interpolated Kneser-Ney}},
year = {2006}
}
@inproceedings{Teh2006,
address = {Morristown, NJ, USA},
author = {Teh, Yee Whye and Whye, Yee},
booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL  - ACL '06},
doi = {10.3115/1220175.1220299},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teh, Whye - 2006 - A hierarchical Bayesian language model based on Pitman-Yor processes.pdf:pdf},
pages = {985--992},
publisher = {Association for Computational Linguistics},
title = {{A hierarchical Bayesian language model based on Pitman-Yor processes}},
year = {2006}
}
@article{Suzuki2016,
abstract = {本研究では，Wikipedia の記事に対して固有表現ク ラスのラベルを自動的に付与するタスクに取り組む． Wikipedia は，最も大規模なオンライン百科事典で ある．誰でも閲覧や編集に参加することができ，扱 われる内容が広範で，更新も活発であることから，言 語資源としての価値が注目されている．一方で，そ の記事は自然言語で書かれているため，必ずしも計 算機で扱いやすいような形式にはなっておらず，構 造化が必要である． 知識の構造化においては，個々のエンティティに 対して「人名」「地名」などといった固有表現クラス のラベルに関する知識を構築することが重要になる． 固有表現クラスは，似た意味的役割を持つ固有表現 をグループ化したクラスであり，このクラスに基づ いてエンティティが持つ属性やエンティティ間に定 義されうる関係を整理した知識ベースは，ファクト イド型質問応答や知識ベースに基づく推論のための 基盤知識として重要である． また，エンティティの固有表現クラスについての 大規模で詳細な知識は，それ単体でも自然言語処理 の様々なタスクにおいて有用であることが知られて いる．Ling ら [7] は 固有表現抽出やエンティティリ ンキングの性能向上に役立つことを指摘している． Wikipedia の情報を構造化して知識ベースを構築す る試みの先駆的なものとして DBpedia [3] と YAGO [10] がある．しかしながら，これらはいずれもオン トロジと記事の対応付けに人手の努力やメタデータ に基づいた単純なルールを用いており，メタデータ が乏しい記事や，新しいクラスへの対応，カバレッ ジの点で課題が残る．また，それら知識ベースがク ラス体系として用いているオントロジも，コミュニ ティベースでボトムアップに設計・維持されていた り，WordNet などの異なるドメインを対象とするオ ントロジを下敷きとしたものが多く，粒度や均一さ の面で課題がある [13]． そこで，我々は，集中的にコントロールされた固 有表現のクラス階層として関根の拡張固有表現階層 1 に着目し，Wikipedia の記事に対して拡張固有表現の ラベルを機械学習により自動で付与することに取り 組む．本研究では，機械学習によるラベル付与にお いて，どのような情報を素性に用いることが有効で あるか，および，適切なラベルを記事に付与する上 でどのようなタスク設計が望ましいかを検討し，実 験を行った． 本研究の貢献は，以下の 2 つである． • 機械学習によって Wikipedia の記事に拡張固有表 現のラベルを付与する上で有効な素性をあらため て検討，設計した．特に，記事のリンク元文脈か ら学習した記事の分散表現を学習の素性として用 いることで，ラベル付与の適合率と再現率がとも に向上することを示した． • それぞれのクラスのラベル付与を独立に学習する のではなく，すべてのクラスのラベル付与を同時 に行う系をニューラルネットワークによって構築 することで，ラベル付与の性能がさらに向上する ことを示した．特に，訓練データに含まれる文書 数が比較的少ないクラスでは分類性能の大きな改 善が見られた． 本研究のタスクの概観図を図 1 に示す．},
author = {正敏, 鈴木 and 耕史, 松田 and 聡, 関根 and 直観, 岡崎 and 健太郎, 乾},
file = {:home/s173342/Downloads/A5-2.pdf:pdf},
journal = {言語処理学会 第22回年次大会 発表論文集},
keywords = {ENE,NLP,Wkikipedia},
mendeley-tags = {ENE,NLP,Wkikipedia},
pages = {797--800},
title = {{Wikipedia 記事に対する拡張固有表現ラベルの多重付与}},
volume = {22},
year = {2016}
}
@misc{Mochihashi2007a,
abstract = {本論文では，n-gram 分布の階層的生成モデルである階層Pitman-Yor 過程を拡張することで，各単語の生まれた隠れたマルコフ過程のオーダを自動的に推定し，適切な文脈を用いる可変長n-gram言語モデルを提案する．無限の深さを持つ予測接尾辞木上の確率過程を考えることにより，句を確率的に発見し，適切な文脈長を学習することができる．これにより，従来不可能だった高次nグラムの学習が可能になる．本手法は言語モデルだけでなく，マルコフモデル一般について，そのオーダをデータから推定できる可変長生成モデルとなっている．英語および日本語の標準的なコーパスでの実験により，提案法の有効性を確認した．},
author = {大地, 持橋 and 英一郎, 隅田},
booktitle = {情報処理学会論文誌},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/大地, 英一郎 - 2007 - 階層Pitman-Yor 過程に基づく可変長n-gram 言語モデル.pdf:pdf},
month = {dec},
number = {12},
pages = {4023--4032},
publisher = {一般社団法人情報処理学会},
title = {{階層Pitman-Yor 過程に基づく可変長n-gram 言語モデル}},
volume = {48},
year = {2007}
}
@misc{Mikolov2013,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
pages = {3111--3119},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@inproceedings{Dagan1993,
address = {Morristown, NJ, USA},
author = {Dagan, Ido and Marcus, Shaul and Markovitch, Shaul},
booktitle = {Proceedings of the 31st annual meeting on Association for Computational Linguistics  -},
doi = {10.3115/981574.981596},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dagan, Marcus, Markovitch - 1993 - Contextual word similarity and estimation from sparse data.pdf:pdf},
pages = {164--171},
publisher = {Association for Computational Linguistics},
title = {{Contextual word similarity and estimation from sparse data}},
year = {1993}
}
@article{Shirotsuka2002,
author = {城塚音也 and 北内, 啓},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/城塚音也, 北内 - 2002 - 文ベクトル集合モデルに基づく文書類似尺度の評価.pdf:pdf},
issn = {09196072},
journal = {情報処理学会研究報告データベースシステム（DBS）},
number = {41},
pages = {159--164},
publisher = {一般社団法人情報処理学会},
title = {文ベクトル集合モデルに基づく文書類似尺度の評価},
volume = {2002},
year = {2002}
}
@article{Kanaya2003,
author = {敦志, 金谷 and 恭司, 梅村},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/敦志, 恭司 - 2003 - 相関係数を用いた実証的重みの分析と検索質問拡張.pdf:pdf},
journal = {情報処理学会研究報告情報学基礎（FI）},
keywords = {相関係数を用いた実証的重みの分析と検索質問拡張　ｐｄｆ},
month = {nov},
number = {112(2003-FI-073)},
pages = {17--24},
title = {相関係数を用いた実証的重みの分析と検索質問拡張},
volume = {2003},
year = {2003}
}
@article{Milne2013,
abstract = {The online encyclopedia Wikipedia is a vast, constantly evolving tapestry of interlinked articles. For developers and researchers it represents a giant multilingual database of concepts and semantic relations, a potential resource for natural language processing and many other research areas. This paper introduces the Wikipedia Miner toolkit, an open-source software system that allows researchers and developers to integrate Wikipediaʼs rich semantics into their own applications. The toolkit creates databases that contain summarized versions of Wikipediaʼs content and structure, and includes a Java API to provide access to them. Wikipediaʼs articles, categories and redirects are represented as classes, and can be efficiently searched, browsed, and iterated over. Advanced features include parallelized processing of Wikipedia dumps, machine-learned semantic relatedness measures and annotation features, and XML-based web services. Wikipedia Miner is intended to be a platform for sharing data mining techniques.},
author = {Milne, David and Witten, Ian H.},
doi = {10.1016/J.ARTINT.2012.06.007},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Milne, Witten - 2013 - An open-source toolkit for mining Wikipedia.pdf:pdf},
issn = {0004-3702},
journal = {Artificial Intelligence},
month = {jan},
pages = {222--239},
publisher = {Elsevier},
title = {{An open-source toolkit for mining Wikipedia}},
volume = {194},
year = {2013}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
month = {jan},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@article{Rong2014,
abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model.},
archivePrefix = {arXiv},
arxivId = {1411.2738},
author = {Rong, Xin},
eprint = {1411.2738},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rong - 2014 - word2vec Parameter Learning Explained.pdf:pdf},
month = {nov},
title = {{word2vec Parameter Learning Explained}},
year = {2014}
}
@inproceedings{Sakurai2008,
abstract = {In this paper, we propose a method for building a general ontology from Japanese Wikipedia to decrease the cost for building and updating general ontology. We take Wikipedia as a semi-structured resource with wide-range concept coverage and new concept coverage. For building is-a relation, we apply string matching to category tree. To exploit more instances, we apply a scraping algorithm to list articles. Case studies show us that a general ontology can be build well from Wikipedia.},
author = {桜井, 慎弥 and 手島, 拓也 and 石川, 雅之 and 森田, 武史 and 和泉, 憲明 and 山口, 高平},
booktitle = {The 22nd Annual Conference of the Japanese Society for Artificial Intelligence, 2008},
file = {:home/s173342/Downloads/IPSJ-NL09194004.pdf:pdf},
pages = {3--6},
title = {{日本語 Wikipedia からの汎用オントロジーの構築と評価 Building up a General Ontology from Japanese Wikipedia}},
year = {2008}
}
@article{Mori2005,
author = {Mori, Junichiro and Matsuo, Yutaka and Ishizuka, Mitsuru},
doi = {10.1527/tjsai.20.337},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mori, Matsuo, Ishizuka - 2005 - Personal Keyword Extraction from the Web.pdf:pdf},
issn = {1346-0714},
journal = {Transactions of the Japanese Society for Artificial Intelligence},
keywords = {keyword extraction,metadata,occurrence,search engine,social network,word co},
number = {5},
pages = {337--345},
publisher = {一般社団法人 人工知能学会},
title = {{Personal Keyword Extraction from the Web}},
volume = {20},
year = {2005}
}
@article{Ueda2009,
author = {Ueda, Hiroshi and Murakami, Harumi and Tatsumi, Shoji},
doi = {10.5687/iscie.22.229},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ueda, Murakami, Tatsumi - 2009 - Extracting Vocation-Related Information for Distinguishing Different People with Identical Names on the.pdf:pdf},
issn = {1342-5668},
journal = {Transactions of the Institute of Systems, Control and Information Engineers},
keywords = {Web person search,information extraction,person name disambiguation,related information,vocation},
number = {6},
pages = {229--240},
publisher = {THE INSTITUTE OF SYSTEMS, CONTROL AND INFORMATION ENGINEERS (ISCIE)},
title = {{Extracting Vocation-Related Information for Distinguishing Different People with Identical Names on the Web}},
volume = {22},
year = {2009}
}
@article{Sekine2018,
abstract = {Wikipedia に書かれている世界知識を計算機が扱えるような形に変換することを目的として、Wikipediaを構造化するプロジェクトを推進している。すでにWikipedia の約 73 万項目を 200 種類の拡張固有表現に分類したデータが完成している。このデータをもとに、それぞれの拡張固有表現で定義された属性を個々の項目の説明文やインフォボックスから抽出し、構造化したデータを作成している。本データの分類部分は[関根ら 18][鈴木ら16][Suzuki et al. 18]に報告した。本論文では、構造化データ作成についての現状を報告する。},
author = {関根 聡 and 小林 暁雄 and 安藤 まや and 馬場 雪乃},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/関根 et al. - 2018 - Wikipedia 構造化データ「森羅」構築に向けて.pdf:pdf},
journal = {言語処理学会 第24回年次大会 発表論文集},
month = {3},
pages = {pp.765--768},
title = {{Wikipedia 構造化データ「森羅」構築に向けて}},
year = {2018}
}
@article{Nakayama2009,
  title={Wikipedia マイニング},
  author={中山浩太郎 and 伊藤雅弘 and 白川真澄 and 道下智之 and 原隆浩 and 西尾章治郎},
  journal={人工知能学会論文誌},
  volume={24},
  number={6},
  pages={549--557},
  year={2009},
  publisher={一般社団法人 人工知能学会}
}
@article{Watanabe2008,
author = {Watanabe, Yotaro and Asahara, Masayuki and Matsumoto, Yuji},
doi = {10.1527/tjsai.23.245},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watanabe, Asahara, Matsumoto - 2008 - Graph-structured Conditional Random Fields for Named Entity Categorization in Wikipedia.pdf:pdf},
issn = {1346-8030},
journal = {Transactions of the Japanese Society for Artificial Intelligence},
keywords = {collective classification,conditional random fields,named entity acquisition},
number = {4},
pages = {245--254},
publisher = {一般社団法人 人工知能学会},
title = {{Graph-structured Conditional Random Fields for Named Entity Categorization in Wikipedia}},
volume = {23},
year = {2008}
}
@article{Tamagawa2011,
author = {Tamagawa, Susumu and Morita, Takeshi and Yamaguchi, Takahira},
doi = {10.1527/tjsai.26.504},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamagawa, Morita, Yamaguchi - 2011 - Building up Ontologies with Many Properties from Japanese Wikipedia.pdf:pdf},
issn = {1346-8030},
journal = {Transactions of the Japanese Society for Artificial Intelligence},
keywords = {ontology,ontology learning,property,wikipedia},
number = {4},
pages = {504--517},
publisher = {一般社団法人 人工知能学会},
title = {{Building up Ontologies with Many Properties from Japanese Wikipedia}},
volume = {26},
year = {2011}
}
@article{Yamamoto2002,
author = {山本, 英子 and 梅村, 恭司},
doi = {10.5715/jnlp.9.2_45},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/山本, 梅村 - 2002 - コーパス中の一対多関係を推定する問題における類似尺度.pdf:pdf},
journal = {自然言語処理},
keywords = {Data Mining},
mendeley-tags = {Data Mining},
number = {2},
pages = {45--75},
title = {コーパス中の一対多関係を推定する問題における類似尺度},
volume = {9},
year = {2002}
}
@inbook{Manning2008,
author={Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze},
title={Introduction to Information Retrieval},
publisher={Cambridge University Press},
year={2008},
pages={260},
isbn={0521865719}
}
@article{Gruber1993,
abstract = {To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse—definitions of classes, relations, functions, and other objects—is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms.},
author = {Gruber, Thomas R.},
doi = {10.1006/knac.1993.1008},
file = {:home/eval/Downloads/ontolingua-kaj-1993.pdf:pdf},
issn = {10428143},
journal = {Knowledge Acquisition},
month = {6},
number = {2},
pages = {199--220},
publisher = {Academic Press},
title = {{A translation approach to portable ontology specifications}},
volume = {5},
year = {1993}
}
@inproceedings{Sekine2018a,
author = {聡, 関根 and 暁雄, 小林 and まや, 安藤 and 耕史, 松田 and 正敏, 鈴木 and Nguyen, Duc and 健太郎, 乾},
booktitle = {言語処理学会 第24回年次大会 発表論文集},
file = {:home/eval/Downloads/P4-5.pdf:pdf},
pages = {504--507},
title = {{「拡張固有表表現＋ Wikipedia 」データ （ 2015年 11 月版 Wikipedia 分類作業完成版）}},
year = {2018}
}
@article{Wang2009,
author = {Wang, Pu and Hu, Jian and Zeng, Hua-Jun and Chen, Zheng},
doi = {10.1007/s10115-008-0152-4},
file = {:home/s173342/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2009 - Using Wikipedia knowledge to improve text classification.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
month = {jun},
number = {3},
pages = {265--281},
publisher = {Springer-Verlag},
title = {{Using Wikipedia knowledge to improve text classification}},
volume = {19},
year = {2009}
}
@inproceedings{Sekine2008,
abstract = {Named Entities (NE) are regarded as an important type of semantic knowledge in many natural language processing (NLP) applications. Originally, a limited number of NE categories were proposed. In MUC, it was 7 categories - people, organization, location, time, date, money and percentage expressions. However, it was noticed that such a limited number of NE categories is too small for many applications. The author has proposed Extended Named Entity (ENE), which has about 200 categories (Sekine and Nobata 04). During the development of ENE, we noticed that many ENE categories have specific attributes, and those provide very important information for the entities. For example, “rivers” have attributes like “source location”, “outflow”, and “length”. Some such information is essential to “knowing about” the river, while the name is only a label which can be used to refer to the river. Also, such attributes are important information for many NLP applications. In this paper, we report on the design of a set of attributes for ENE categories. We used a bottom up approach to creating the knowledge using a Japanese encyclopedia, which contains abundant descriptions of ENE instances.},
author = {Sekine, Satoshi},
booktitle = {Proceedings of the Sixth International Language Resources and Evaluation (LREC'08)},
doi = {citeulike-article-id:3043436},
isbn = {2-9517408-4-0},
title = {{Extended Named Entity Ontology with Attribute Information}},
year = {2008}
}
@inproceedings{Kohavi1995,
 author = {Kohavi, Ron},
 title = {A Study of Cross-validation and Bootstrap for Accuracy Estimation and Model Selection},
 booktitle = {Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2},
 series = {IJCAI'95},
 year = {1995},
 isbn = {1-55860-363-8},
 location = {Montreal, Quebec, Canada},
 pages = {1137--1143},
 numpages = {7},
 acmid = {1643047},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
@article{Kikuchi2019,
                  author = {真人, 菊地 and 賢人, 川上 and 光男, 吉田 and 恭司, 梅村},
                  title = {観測頻度に基づく尤度比の保守的な直接推定法},
                  journal = {電子情報通信学会論文誌D (採録決定)},
                  year = {2019}
}
@inproceedings{Wu2008,
 author = {Wu, Fei and Weld, Daniel S.},
 title = {Automatically Refining the Wikipedia Infobox Ontology},
 booktitle = {Proceedings of the 17th International Conference on World Wide Web},
 series = {WWW '08},
 year = {2008},
 isbn = {978-1-60558-085-2},
 location = {Beijing, China},
 pages = {635--644},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1367497.1367583},
 doi = {10.1145/1367497.1367583},
 acmid = {1367583},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {markov logic networks, ontology, semantic web, wikipedia},
}
@inproceedings{Sultana2012,
 author = {Sultana, Afroza and Hasan, Quazi Mainul and Biswas, Ashis Kumer and Das, Soumyava and Rahman, Habibur and Ding, Chris and Li, Chengkai},
 title = {Infobox Suggestion for Wikipedia Entities},
 booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
 series = {CIKM '12},
 year = {2012},
 isbn = {978-1-4503-1156-4},
 location = {Maui, Hawaii, USA},
 pages = {2307--2310},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2396761.2398627},
 doi = {10.1145/2396761.2398627},
 acmid = {2398627},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {text classification, wikipedia},
}
@inproceedings{Auer2007,
 author = {Auer, S\"{o}ren and Lehmann, Jens},
 title = {What Have Innsbruck and Leipzig in Common? Extracting Semantics from Wiki Content},
 booktitle = {Proceedings of the 4th European Conference on The Semantic Web: Research and Applications},
 series = {ESWC '07},
 year = {2007},
 isbn = {978-3-540-72666-1},
 location = {Innsbruck, Austria},
 pages = {503--517},
 numpages = {15},
 url = {http://dx.doi.org/10.1007/978-3-540-72667-8_36},
 doi = {10.1007/978-3-540-72667-8_36},
 acmid = {1419709},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}
@inproceedings{Bing2013,
 author = {Bing, Lidong and Lam, Wai and Wong, Tak-Lam},
 title = {Wikipedia Entity Expansion and Attribute Extraction from the Web Using Semi-supervised Learning},
 booktitle = {Proceedings of the Sixth ACM International Conference on Web Search and Data Mining},
 series = {WSDM '13},
 year = {2013},
 isbn = {978-1-4503-1869-3},
 location = {Rome, Italy},
 pages = {567--576},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2433396.2433468},
 doi = {10.1145/2433396.2433468},
 acmid = {2433468},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {entity expansion,, information extraction, proximate record graph, semi-supervised learning},
} 
@inproceedings{Hao2011,
 author = {Hao, Qiang and Cai, Rui and Pang, Yanwei and Zhang, Lei},
 title = {From One Tree to a Forest: A Unified Solution for Structured Web Data Extraction},
 booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '11},
 year = {2011},
 isbn = {978-1-4503-0757-4},
 location = {Beijing, China},
 pages = {775--784},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2009916.2010020},
 doi = {10.1145/2009916.2010020},
 acmid = {2010020},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {information extraction, site-level information, structured data, vertical knowledge},
} 